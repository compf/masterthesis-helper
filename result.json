{
  "NumberWordsPlusLettersMetric SentenceSplitter": [
    {
      "score": 321,
      "sentence": "\n\\newline\n\nBest regards,\\newline\nTimo Schoemaker \\newline\nDepartment of Computer Science \\newline\nUniversity of Osnabr端ck \\newline\n\\clearpage\n\\chapter{UML diagram of the tool}\nFigure \\ref{fig:uml_classes} depicts the most important classes of the tool in the scenario that data clumps are detected manually and the \\ac{LLM} performs the refactoring"
    },
    {
      "score": 290,
      "sentence": "\n\n\n\\subsubsection{Running the main program}\n\n\nIf a valid configuration file has been created, the main program can be started via \\textit{npm run run <config> <project-location>} where \\textit{config} is the location of the configuration file and \\textit{project-location} is the path to the project to be analyzed"
    },
    {
      "score": 275,
      "sentence": "\n\n\nConversely, it will be tested whether ChatGPT can execute the refactoring itself by providing the pieces of the source code containing data clumps of a software project and providing specific queries to find the data clumps, refactor them, and output the refactored source code"
    },
    {
      "score": 274,
      "sentence": "\n\nAs a result, in the \\textit{one-data-clump approach}, a data clump is processed by one step of the pipeline and then by the next while in the \\textit{one-step approach} the first step has to process all data clumps first before the data clumps can be processed by the succeeding steps"
    },
    {
      "score": 272,
      "sentence": " \n\n\nIt should be noted that the order of the handlers in the configuration  does not matter because the execution order is constant and, in most cases, each step requires the context of a previous step so that parallel execution or vice-versa execution of steps is not possible"
    },
    {
      "score": 272,
      "sentence": " In the refactoring step only two random data clumps are considered  because it can be expected that subsequent validation steps would increase the amount of transmitted data even more, so that submitting more files would increase the likelihood of context window overflow"
    },
    {
      "score": 268,
      "sentence": " Details of these more complex filtering methods can be found in section \\ref{sec:data_clump_filtering}\n\\subsubsection{Extraction of AST}\nThen,  the source code needs to be simplified as comments, the method bodies, and other language parts are irrelevant for detecting data clumps"
    },
    {
      "score": 261,
      "sentence": " This rejects their hypothesis that data clumps do not affect faults, and the authors suggest that the application domain and the development context need to be considered before the time-consuming refactoring of data clumps since their impact is not predictable"
    },
    {
      "score": 261,
      "sentence": " \n\nThe following subsections presents some  criteria for filtering data clumps\n\\subsubsection{Ignore abstract methods}\n\nAbstract methods or non-implemented methods in interfaces only describe a contract without functionality which must be implemented by derived classes"
    },
    {
      "score": 260,
      "sentence": " For instance, choosing between \\enquote{strongly disagree} and \\enquote{disagree} might be simpler or more complex than deciding between \\endquote{strongly agree} and '\\enquote{agree} \\cite{HARPE2015836}\n\n\nFor this master thesis, the survey platform \\textit{LamaPoll} \\cite{lamapoll} was used"
    }
  ],
  "NumberWordsPlusLettersMetric ParagraphSplitter": [
    {
      "score": 1857,
      "sentence": "%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{acronym}[E/E/PE] %sorgt fuer proper indention\n\t\\acro{API}{\\emph{Application Programming Interface}}\n\t\\acro{AST}{\\emph{Abstract Syntax Tree}}\n\t\\acro{ATL}{\\emph{Atlas Transformation Language}}\n\t\\acro{BMWi}{\\emph{Bundesministerium f端r Wirtschaft und Energie}}\n\t\\acro{CIM}{\\emph{Computation-Independent Model}}\n\t\\acro{CDC}{\\emph{Code-level design choice}}\n\t\\acro{CR}{\\emph{Code-level requirement}}\n\t\\acro{CI/CD}{\\emph{Continuous Integration/Continuous Delivery}}\n\t\\acro{CRC}{\\emph{Cycling Redundancy Checks}}\n\t\\acro{E/E/PE}{\\emph{Electrical/Electronic/Programmable Electronic}}\n\t\\acro{ECC}{\\emph{Error Detecting and Correcting Codes}} \n\t\\acro{EMF}{\\emph{Eclipse Modeling Framework}}\n\t\\acro{EGL}{\\emph{Epsilon Generation Language}}\n\t\\acro{EOL}{\\emph{Epsilon Object Language}}\n\t\t\\acro{HTML}{\\emph{Hyper Text Markup Language}}\n  \t\t\\acro{HTTP}{\\emph{Hyper Text Transfer Protocol}}\n\t\\acro{Epsilon}{\\emph{Extensible Platform of Integrated Languages for mOdel maNagement}}\n\t\\acro{FS}{\\emph{Functional Safety}}\n\t\\acro{HAL}{\\emph{Hardware Abstraction Layer}}\n\t\\acro{HolMES}{\\emph{Holistische Modell-getriebene Entwicklung f端r Eingebettete Systeme unter Ber端cksichtigung unterschiedlicher Hardware-Architekturen}}\n\t\\acro{IDE}{\\emph{Integrated Development Environment}}\n\t\\acro{JSON}{\\emph{JavaScript Object Notation}}\n\t\\acro{JDT}{\\emph{Java Development Tools}}\n\t\n\t\\acro{LOC}{\\emph{Lines of Code}}\n \t\\acro{LLM}{\\emph{Large Language Model}}\n    \\acro{LSP}{\\emph{Language Server Protocol}}\n\t\\acro{MISRA}{\\emph{Motor Industry Software Reliability Association}}\n\t\\acro{MBU}{\\emph{Multi Bit Upset}}\n\t\\acro{MDA}{\\emph{Model Driven Architecture}}\n\t\\acro{MDC}{\\emph{Model-level design choice}}\n\t\\acro{MDD}{\\emph{Model Driven Development}}\n\t\\acro{MDE}{\\emph{Model Driven Engineering}}\n\t\\acro{MOF}{\\emph{Meta Object Facility}}\n\t\\acro{MR}{\\emph{Model-level requirement}}\n\t\\acro{NLP}{\\emph{Natural Language Processing}}\n\t\\acro{OCL}{\\emph{Object Constraint Language}}\n\t\\acro{OMG}{\\emph{Object Management Group}}\n\t\\acro{PIM}{\\emph{Platform-Independent Model}}\n \\acro{PSI}{Program Structure Interface}\n\t\\acro{PSM}{\\emph{Platform-Specific Model}}\n\t\\acro{SER}{\\emph{Soft Error Rate}}\n\t\\acro{SEU}{\\emph{Single Event Upset}}\n\t\\acro{TMR}{\\emph{Triple Modular Redundancy}}\n\t\\acro{UML}{\\emph{Unified Modeling Language}}\n\t\\acro{VCS}{\\emph{Version Control System}}"
    },
    {
      "score": 1366,
      "sentence": "One main finding on the evaluation results is the importance of selecting suitable data clumps for refactoring. In many cases, the developers believe that the suggested refactoring is not worth it, makes the code harder to read or the performance implication are too negative. This shows that data clumps are a code smell where there is a wide disagreement on when refactoring is warranted and when not. because all tested projects have a multitude of different data clumps that have various sizes, affected files or included variables, it is very difficult to select a suitable data clump. Traditional metrics like the size or the occurrence seem to have only a partial influence on the acceptance of data clump refactoring. Instead, the relationship of the variables is more often a better indicator. Here, the model can be a great help as it does not necessarily rely on these common metrics. However, as it can bee seen, the choice by the model is often not enough. Instead, the inherent knowledge of the contributors of a project is a major factor in  deciding whether refactoring is warranted. Only with information about the scope, goals, and issues of a project, it is possible to determine where refactoring is more likely to be warranted. Even with the help of \\ac{LLM}, external contributions can hardly mitigate this lack of knowledge which explains many pull request rejections. "
    },
    {
      "score": 1233,
      "sentence": "With no human in the loop, the output of the \\ac{LLM} must be parsed automatically and be used by the respective handler to perform the current pipeline step. Therefore, defining a suitable output  format  is essential as the handler has to rely on the output and cannot adequately deal with an output that does not conform to the specified format. \n\\subsubsection{Detection}\nIf the \\ac{LLM} is asked to detect data clumps, the format described in appendix \\ref{app:data_clump_format} can be used to ease compatibility to other tools. This form can be clearly defined as it can be easily parsed. A disadvantage of this format is that it can become verbose and contain redundant information. For instance, if four methods  constitute a data clump, there will be six entries in the output format (i.~e. method a-b, a-c, a-d, b-c, b-d, c-d). All these entries must, according to the format, contain the class name, the file path, a unique id, the names and types of the parameters etc. This redundancy is superb for manual refactoring tools as this information can be easily accessed. For instance, a refactoring tool can change the signature of one method independently of another because all relevant information is located within the data clump information for that particular method. "
    },
    {
      "score": 1217,
      "sentence": "Moreover, there are code smells that remain often unnoticed and hence unfixed because they are harder to detect and fix. One particular code smell named \\textbf{data clumps} is the focus of this master thesis. Data clumps refer to duplicated fields or method parameters across the source code. This duplication increases the code size and makes the source code harder to read because these variables have a implicit connection that is not readily apparent.  This code smell can be difficult to detect as it can span multiple files and requires more extensive analysis of the source code~\\cite{BaumgartnerAP23}~\\cite{data_clumps_refactoring_guru}~\\cite{join_data_items}.\n \nSeveral studies deal with data clumps. They are one of the top ten most-cited code smells~\\cite{lacerda} and are recommended for low-priority refactorization~\\cite{zhangPrioritisingRefactoringUsing2011}. The impact of data clumps on software quality is not conclusive, but research suggests that the \\enquote{application domain and development context} influence whether data clumps are problematic or not~\\cite{hallCodeSmellsHave2014}. This highlights that not every data clump is a code smell per se. However,  identifying and refactoring some of them can have a positive impact on the software quality. "
    },
    {
      "score": 1092,
      "sentence": "Figure \\ref{fig:margin_effect} illustrates the advantage of a larger margin size. The source code shows counters used for testing purposes and a possible way to reset them \\footnote{This example is somewhat inspired by a JUnit5 data clump that was refactored as part of the evaluation in section \\ref{sec:pull_request_eval}}. The first four fields are part of a data clump (the second class is not shown for brevity). Assume for the sake of this example that only the data clump item \\enquote*{afterAll} is transmitted. If the margin is zero, only line 4 would be transmitted to the model (black rectangle). This means that the \\ac{LLM} has only a scarce overview over the purpose of the variable.\nThe green rectangle covers the area if a margin of one is used. Now, line 3 and 5 are also included. Since line 5 mentions something about tests, the model can infer where the data clump items are used and might improve its refactoring. The red rectangle represents a margin size of three. In this case, the full code block is transmitted, and the model can observe that it might be useful to include a reset method in the extracted class. "
    },
    {
      "score": 1040,
      "sentence": "Other models will forget some parts of it even though they might be relevant for the use case of the \\ac{LLM}. Figure \\ref{fig:llm_loose_context} demonstrates the issues that arises.  A model is instructed to find and refactor data clumps in java files, and those java files are submitted as well. The response of the model at the bottom of the figure is however unexpected as it only explains the contents of the files submitted. The reason for this response is not some misunderstanding of the prompt but a lack of prompt. The transparent rectangle represents the context window, only covering the content of the transmitted files while the instruction is not covered. \nBecause of the context window limitation, transmitting the third file overwrites the content of the prompt so that the model forgets the initial instruction and cannot reasonably process the files, giving instead a description of the files. The problem also arises on greater context window if multiple files or large files are involved as it may be the case while refactoring data clumps."
    },
    {
      "score": 1033,
      "sentence": "To use an \\ac{LLM} effectively, understanding its operation and correct usage is crucial for better results. This section outlines the relevant background about \\acp{LLM}. In the beginning, section \\ref{sec:llm_introduction} outlines the technical background of \\acp{LLM}. Afterwards, section  \\ref{sec:chatgpt}  describes how ChatGPT can be used via the \\ac{API} whose structure will be used as the foundation for a general \\ac{LLM}-\\ac{API} in later chapters. ChatGPT is contrasted to other alternative models in section \\ref{sec:other_llm} highlighting that despite the popularity of ChatGPT, these options deserve consideration but have weaknesses too. In the remaining sections, weaknesses and advantages of \\acp{LLM} in general are discussed. In section \\ref{sec:llm_challenges}, the advantages and disadvantages of using \\ac{LLM} are compared giving an overview of what arguments exist on both sides. The succeeding section \\ref{sec:llm_considerations} discusses some problems that primarily arise if models are wrongly used and outlines how the quality of these models can be maximized. "
    },
    {
      "score": 973,
      "sentence": "Alternatively, a replace method can be used. This means that the string \\textit{oldContent} is replaced by \\textit{newContent} without considering the line number information. This method prevents the issues of the first approach. Additionally, it might refactor more lines that the \\ac{LLM} did not refactor for some reasons. This can have advantages but also challenges. For instance, an instruction to replace the curly bracket (\\enquote*{\\{}) by an empty string can have fatal implication in a java source code file as it will contain many of such braces and removing them all will lead to non-compilable programs. Additionally, this does not work if \\textit{oldContent} for some reason does not exist in the file. For instance, the model might have added extra whitespaces to the old content so that a simple search-and-replace-strategy does not work. Furthermore, if oldContent is an empty string, replacing can lead to file corruption as the replacement operations are not developed for such an scenario. "
    },
    {
      "score": 954,
      "sentence": "Another problem is the creation of those extracted classes is object instantiation. Suppose a method previously requires three parameters \\textit{x}, \\textit{y}, and \\textit{z}. For instance, consider a method that initially takes three parameters: \\textit{x}, \\textit{y}, and \\textit{z}. After refactoring, this method would need just one parameter, an instance of the data class \\textit{Point}, meaning callers must now create a \\textit{Point} instance instead of providing three separate arguments. This could lead to the creation of many instances, consuming more memory and necessitating frequent garbage collection, thereby reducing performance. While these effects might be minimal, the effect is more significant on embedded systems or other systems with few resources. A similar problem occurs if the access to variables is replaced by getters or setters which also creates some overhead. While many compilers are capable of optimizing such issues, they should nevertheless be considered. "
    },
    {
      "score": 946,
      "sentence": "Murphy et al. \\cite{stench_blossom} developed a visual tool to find code smells (including data clumps). the tool named \\textit{Stench Blossom} is a plugin for Eclipse. If a plugin user views a file, the tool searches for code smells and displays them at the right side of the Eclipse editor. Each code smell is visually displayed as a a \\textit{petal}. The radius of such petal represents the strength of a smell (i.~e. how strongly violates the code smell the given coding guidelines). The petal has a color that indicates how obvious a code smell is. Orange code smells are not very obvious while blue code smells are easy to detect by human beings. Since orange has a closer connection to warnings, it is more easily to perceive so that undetected code smells can be fixed better. Experiments with \\textit{Stench Blossom} shows that the median number of code smells detected by humans using the tool increased from 11 to 21. However, data particular for data clumps is not given."
    }
  ],
  "NumberWordsPlusLettersMetric LatexSectionSplitter": [
    {
      "score": 3960,
      "sentence": "API}\nIn order to utilize the ChatGPT \\ac{API}, a user has to create an OpenAI account and provide financial information for billing purposes. Each request to the API consumes a certain amount of tokens dependent on the query. \n\nA token is the smallest unit used by ChatGPT to process a query. For instance, a token could be one English word, a syntactical part of a programming language, a number, or a similar discrete segment of a query. According to OpenAI, a token roughly equates 3/4 of a word, so 100 tokens are about 75 words. However, this is just an estimate for natural language texts in the English language. \n\nThe \\ac{API} itself used JSON combined with \\ac{HTTP}. For the purpose of this master thesis, only a subset of the available means to use the \\ac{API} will be explained because the whole \\ac{API} would be too complex and outside the scope this master thesis. \n\nListing \\ref{lst:chatgpt_api} shows how an \\ac{API} query may look like if it used via \\textit{curl} which is a tool to  perform \\ac{HTTP} requests:\n\n \\begin{figure} [htbp!]\n \\centering\n\t\t\t\\scalebox{0.8}{\\lstinputlisting\n\t\t\t[caption={ Example query to ChatGPT. based on ~\\cite{ChatGPT_url}}, \n\t\t\tlabel={lst:chatgpt_api},\n\t\t\tcaptionpos=b,language=java, basicstyle=\\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]\n\t\t\t{figures/chapter2/chatgpt_api.json}}\n\t\t\\end{figure}\n \n\nSince ChatGPT uses \\ac{JSON} for communication, the content header of \\ac{HTTP}  must be set to \\enquote{application/json} (l. 2). Subsequently, a token must be provided to ChatGPT (l. 3). which is generated on the OpenAI website and links the query to a specific OpenAI account for billing purposes. It is crucial that this token remains confidential and is not exposed (e.~g., through version control systems).\n\nThen, the actual query is defined (l. 4-24.). Firstly, the model is defined (l. 5). OpenAI provides multiple models that have different advantages and disadvantages. For instance, a newer model like \\textit{gpt-4} has more capabilities but is more expensive.  By providing a temperature, a user can configure the randomness of the response (l.6). A higher temperature results in more creative answers but this creativity might worsen the quality of the responses. \n\nAfterward, the actual messages are provided (l. 7-25). Each message is a tuple of a \\textbf{role} and a \\textbf{content}. The content of a message is the input or output provided to or by ChatGPT. \n\nThe role of a message indicates the source of a message. If the content of the messages comes from a user, the role should be \\enquote{user} (l. 13). A reply for a query is defined as the role \\enquote{assistant} (l. 17). The system role is a more specific and can be used to include further context for ChatGPT without eliciting a response (l. 9).\n\nThe response of the ChatGPT \\ac{API} based on listing \\ref{lst:chatgpt_api} is displayed in listing \\ref{lst:chatgpt_api_response}.\n \\begin{figure} [htbp!]\n \\centering\n\t\t\t\\scalebox{0.8}{\\lstinputlisting\n\t\t\t[caption={The response from ChatGPT. Based on ~\\cite{ChatGPT_url}},\n\t\t\tlabel={lst:chatgpt_api_response},\n\t\t\tcaptionpos=b,language=java, basicstyle=\\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]\n\t\t\t{figures/chapter2/chatgpt_api_response.json}}\n\t\t\\end{figure}\n\nIn the bottom part of the listing (l.~13-19), meta information is provided. This includes the response generation time (l.~12), the used model for the response (l.~14), and an id for the response (l.~13).\n\nTo enable clients to calculate the costs of using ChatGPT, each response also includes how many tokens have been used by the prompt (l.~18), by the response (l.~17), and the total number of tokens (l.~19).\n\nIn the upper part of the response (l. 1-11), the actual response is provided. The response is an array of so-called choices (l. 2~-~11). Each choice consists of the actual message (l.~ 6-8), which itself consists of the message content (l. 7) and the role of the content (usually \\enquote{assistant}).\n\nThe \\enquote{finish\\_reason} indicates how the \\ac{LLM} has finished on the prompt. If the value is \\enquote{stop}, the prompt was executed without faults so that the response is valid. If the value is \\enquote{length}, the output would exceed the maximum token limit, so the output will be incomplete. The value \\enquote{content\\_filter} indicates that OpenAI censured the requests because it violates the terms of use of OpenAI.~\\cite{ChatGPT_url}\n\n\n\\sub"
    },
    {
      "score": 3938,
      "sentence": "Disadvantages}\n\nFirst of all, \\acp{LLM} are not trustworthy. They are often confident in their answers which nevertheless are wrong. This confidence can often be broken by asking subsequent questions which lead the \\ac{LLM} to rethink the answer. however, doing this in an automatic way is challenging.~\\cite{azaria2023internal}\n\nAdditionally, \\acp{LLM} use randomness in their answers which means that the same query can result in different replies. The factors influencing the reply are generally not known and should not be assumed. As a result, requirements regarding a specific output format may be ignored by the model so that developers using a \\ac{LLM} must always consider how to parse non-adhering output. ~\\cite{hu2023large}\n\nOne issue that many \\acp{LLM} share is hallucination. This means that the model generates a response, but does not come to an end. For instance, an \\ac{LLM} generating \\ac{JSON} can add more and more \\ac{JSON} objects to its response until it has no space left. As a result, it must abruptly stop leaving an invalid \\ac{JSON} with no closed braces.  The conditions when such behavior can be observed are unpredictable and should be taken care for. \n\nFurthermore, \\ac{LLM}s are usually black boxes. They do not give hindsight on how they came to a specific reply. While they can explain their reasoning, it is not possible to check the exact thought process.\nWhile a query can consist of multiple parts, conditions, or requirements, a \\ac{LLM} will not always adhere to all of these. It may weigh some requirements, ignore other, or interpret them wrongly so that the result is unexpected. A \\ac{LLM} may also come to an intermediate result that it will not show at the end even though the intermediate result was correct or requested. Also, no sources of the information is provided.~\\cite{chen2023instructzero}\n\nMoreover, \\ac{LLM}s do not have access to the latest information about a topic. They cannot access external sources like current news and up-to-date documentation. Instead, they employ a so-called cut-off date. Only information before that cut-off date will be used. As of the time of writing this section, the cut-off date for ChatGPT is April  2023. However, the release was several months later.  \n\nThere are also security issues with using  \\ac{LLM} like ChatGPT. If a model is asked to generate or refactor code, one cannot trust that the code is safe to use. As a result of the cut-off date, the code might use operations that are considered deprecated or even unsafe to use because security vulnerabilities have been detected in the meantime. As a result, the developer needs to verify whether the code is safe to use which is another burden. ~\\cite{pearce2021asleep}\n\nFurthermore, it is not out of the question that a malicious attacker might change the query or the reply of a \\ac{LLM}. Therefore, using such a model might be a feasible way to hack systems or create damage which is difficult to detect and prevent.~\\cite{not_what_you_signed_for}\n\nAnother significant aspect is the legal implications of using \\acp{LLM}. Since these models have only recently become mainstream, there are still unresolved legal issues. For instance, since most \\acp{LLM} are trained using publicly available data, there are copyright issues as the idea of a particular line of code might come from somebody unknown. Also there are responsibility questions. What happens if the output of the model is malicious or faulty and who is legally responsible for making sure that no damage is done if the output of a model is used. The answers to these issues depend strongly on the jurisdiction. For instance, in the European Union the EU AI Act (Regulation (EU) 2024/1689), requires  transparency for using \\acp{LLM} and bans the use of them in situations critical for human safety. ~\\cite{eu-ai-act}\n\n\nLastly, also costs and capacity considerations needs to be observed. For large projects, a \\ac{LLM} might be too costly because  the costs are often dependent on the input size. Therefore, the use of large language models should be adequately prepared so that as much costs as possible can be saved. ~\\cite{chen2023frugalgpt}\n\n\n\n\n\\sub"
    },
    {
      "score": 3291,
      "sentence": "Category assignment}\n\nAfter considering all of these criteria, one final data clump was selected.\nAfter a data clump is selected, the next step is to assign the project to one of two categories to determine the extent \\acs{LLM} are used to find and refactor data clumps. \n\n\n\nIn the first category, ChatGPT performs the refactoring completely. Because transmitting whole GitHub projects would infeasible, the \\textit{DataClumpDoctor} was used to detect the previously selected data clump and obtain all locations of interests. A margin of 5 was used so that 5 lines below and 5 lines above each location of interest were transmitted. \n\nThen, ChatGPT is instructed to refactor all data clumps in the provided locations of interest. This instruction was repeated at least ten times, in each time the context of ChatGPT was cleared so it didn't know its previous answers. \n\nFrom these ten proposals, one proposal was chosen that describes refactoring data clumps most accurately. For instance, the extracted class is valid, most usages of the data clump items are updated and all method signatures are refactored (if applicable). Generating multiple proposals is necessary because not every proposal will be correct.\n\n\n\nThe second approach for refactoring was via IntelliJ. In this case, ChatGPT only suggests a suitable name for the extracted class but is otherwise not involved in the refactoring. Instead, IntelliJ performs all refactoring in the manner described in section  \\ref{sec:intellij_refactoring}. This results in a very consistent refactoring without any creativity. Hence, this refactoring needs only to be executed once and the first proposal can be selected immediately. \n\n\nAfter selecting a proposal, the proposal is applied and saved on a separate branch.\nAfterwards, the proposal might not be fully correct. For instance, there might be  none-updated method calls, missing semicolons etc. An additional problem occurs if code style tools like \\textit{SpotBugs} or \\textit{Checkstyle} are employed. If the refactoring by the \\ac{LLM} does not conform to the required codestyle, the code might not compile because the developers of the project force a certain style. Therefore, a manual correction step is performed. The project is  manually changed in such a way that it fully compiles. However, no creative refactoring is performed. For instance, if one part of the source code was not refactored, it was refactored like another part regardless of whether another refactoring might have make more sense. This reduces human intervention to a minimum and ensures that the creative part of the refactoring is done by the \\ac{LLM}. \n\nAs soon as this manual refactoring finished and the program compiles, the changes were squashed into one commit and a pull request was created in the respective repository. In this pull request, the maintainers were described the purpose of this pull request and  the definition of data clumps used in this master thesis. They were asked to give feedback by filling out a feedback form or by giving feedback via GitHub comments under the respective pull request. It was explicitly stressed that rejecting the pull request would not be perceived negatively. The full text of the pull request can be found in appendix \\ref{app:pr_text}.\n\nFeedback from both the form and comments was collected and evaluated as described in \n section \\ref{sec:feedback_survey}.\n\n\\subsub"
    },
    {
      "score": 3116,
      "sentence": "Name finding}\nFinding a suitable name for the soon-to-be-extracted class is the next step  that must be implemented. Two approaches are chosen.\n\nThe first approach is the classic approach that was used before the advent of \\acs{LLM}. By concatenating all field names of the extracted class, a valid class name can be generated. For instance, a class with the fields \\textit{x}, \\textit{y}, and \\textit{z} could be named \\textit{XYZ}. In this example, the field names are capitalized and directly joined without any separator, however, other options might be better dependent on the project's style guidelines. \n\nThe advantage of this manual method is that the generated names are very unique and with a high probability will not conflict with other names because they are very artificial. Nevertheless, they lack creativity and in most cases a developer will need to choose a better name in order to improve readability. \n\n\nOn the other hand, a suitable name for the extracted class of a data clump can be chosen by using the creativity of \\acs{LLM}.  For this the model has to know the names of the variables of the data clump. It is also useful to include the qualified type of each data clump item because this type contains additional information to generate a more suitable name. For instance, the qualified name could contain the name of the project to analyze, the location of the type,     and the range of possible values that the variable can have.\n\nWhile using \\acs{LLM} for name finding, excessive name caching can be useful to save costs and resources. The same combination of data clump items will probably result in the same class name. In order to prevent the extraction of classes with similar purposes, it is therefore useful to only ask the model if the types-names identifier has not occurred before. \n\nA related issue to name finding is the location of the extracted class. Here, one has to differentiate  between data clumps existing in a single file and data clumps connecting multiple files. In the former case, the location of the extracted class can be an already existing file, namely the file of the data clump  while in the latter case, a new class should be created.\n\nUsing an existing class can be problematic because the succeeding step (class extraction) must be mindful that an existing class is used as it could override the file. Additionally, integrating the new class into the existing class can be challenging. Should the new class be an inner class of some class? In Java, should the inner class be static?  Creating an inner class requires strategic syntactical modification of the source code which would require specialized handlers. As a result, they are not implemented in this master thesis, but could be. \n\nOn the other hand, using a completely new file is easier to implement. However, one has to choose the path of the extracted class. For instance, a data clump might be spread over \\textit{n} files each of which is in a separate directory, thereby creating a theoretical possibility of \\textit{n} candidates as the output directory. \nOne also has to be mindful about any name conflicts that might occur. If the suggested name already exists, it will lead to conflicts.\n\n\n\\sub"
    },
    {
      "score": 2783,
      "sentence": "Research related to data clumps}\n\nAs outlined in section \\ref{sec:data_clump_def}, the definition of data clump by Fowler in \\cite{fowler2019refactoring} is somewhat ambiguous because no clear criteria to determine data clumps is established. Zang et al. \\cite{zhangImprovingPrecisionFowler2008} creates a more algorithmic approach to determine whether a data clump exists. This approach is also explained in section \\ref{sec:data_clump_def}. The authors also provide more precise definitions of other code smells like \\enquote{message chains} or \\enquote{speculative generality}. By interviewing four software development experts about the code smell definitions the authors developed, they find that their new data clump definition receives relatively more disagreement than other definitions. The authors believe  that not covering edge cases is a major reason for this negative feedback. \n\n\nHall et al. analyzed the impact of code smells (including data clump) on the occurrence of faults in three open-source software projects. They find that data clumps have a mixed correlation to faults because, in two of the three projects analyzed, the correlation of data clumps per \\ac{LOC} to detected faults is negative, but positive for another project. This rejects their hypothesis that data clumps do not affect faults, and the authors suggest that the application domain and the development context need to be considered before the time-consuming refactoring of data clumps since their impact is not predictable.  \\cite{hallCodeSmellsHave2014}\n\n\nBaumgartner et al. developed a live code smell detection plugin for IntelliJ that can detect, report, and refactor data clumps without significantly impacting performance. However, the tool is semi-automatic, meaning the developer must still actively approve the data clump refactoring and suggest a suitable class name for the extracted class. \\cite{BaumgartnerAP23}\n\nMurphy et al. \\cite{stench_blossom} developed a visual tool to find code smells (including data clumps). the tool named \\textit{Stench Blossom} is a plugin for Eclipse. If a plugin user views a file, the tool searches for code smells and displays them at the right side of the Eclipse editor. Each code smell is visually displayed as a a \\textit{petal}. The radius of such petal represents the strength of a smell (i.~e. how strongly violates the code smell the given coding guidelines). The petal has a color that indicates how obvious a code smell is. Orange code smells are not very obvious while blue code smells are easy to detect by human beings. Since orange has a closer connection to warnings, it is more easily to perceive so that undetected code smells can be fixed better. Experiments with \\textit{Stench Blossom} shows that the median number of code smells detected by humans using the tool increased from 11 to 21. However, data particular for data clumps is not given.\n\n\n\n\n\\subsub"
    },
    {
      "score": 2588,
      "sentence": "Advantages}\n\nFirst of all, large language models are very flexible. A normal refactoring algorithm needs to consider many situations. For instance, a traditional approach that modifies the method signature in a class might not work on an interface. A large language model does not need to be adapted to all edge cases but often finds a suitable solution to a problem because it is not restricted to a specific refactoring process.~\\cite{shirafuji2023refactoring}\n\nAdditionally, an \\ac{LLM} is more similar to a human as it is more  \\enquote{creative}. While it is still a computer model and does not win the Turing-Test~\\cite{turing_test}, a \\ac{LLM} can refactor code in a manner more closely as a human being would do. For instance, it can suggest class names that are related to the topic of the class, which a human being would also consider, while traditional approaches would use placeholder names, concatenation of field names or other simple name construction algorithms.~\\cite{shirafuji2023refactoring}\n\nLarge language models are also extensible. For instance, if another programming language is used, an \\ac{LLM} can be easily adapted, while a traditional refactoring approach would require more effort to be language-agnostic.\n\nMoreover, an \\ac{LLM} can refactor the code in more ways than instructed. While a model can be specifically instructed to refactor data clumps, it might also correct formatting errors, spelling mistakes, or other code smells. While the focus of this master thesis will be on data clumps, other code smells are important too and might be more serious. Using a  \\ac{LLM} allows developers to fix more code smells without developing and testing more tools to refactor multiple code smells. As they are better to understand the context of the code than a traditional approach, the quality of the code can therefore be improved.~\\cite{shirafuji2023refactoring}\n\nAdditionally, \\acp{LLM} employ a interaction-based learning model which significantly facilitates the integration of developer feedback in real-time. This dynamic learning process allows \\acp{LLM} to progressively refine and optimize their suggestions based on continuous interactions with developers.~\\cite{10.1145/3581641.3584037} \n%Furthermore, \\ac{LLM} can adapt to the coding style of the source code. If for instance, the source code used the \\enquote{snake\\_case} or the \\enquote{pascalCase} naming convention, the model can detect this convention and use it for its own refactoring (e.~g. creating new methods, variables or classes). A traditional approach would need to be configured for each project to use the right convention so that the generated code might look more artificial as it does not not fit to the rest of the code.\n\n\n\\subsub"
    },
    {
      "score": 2505,
      "sentence": "Discussion}\nThe results show that surveys via GitHub pull request pose some challenges. Some pull requests are left unanswered, while others are closed without comments or the comment contains very general feedback and not helpful for evaluating the survey. Additionally, the tendency of developers to use GitHub for giving feedback  instead of the suggested survey platform complicates a solid analysis of the evaluation.\n\nOne main finding on the evaluation results is the importance of selecting suitable data clumps for refactoring. In many cases, the developers believe that the suggested refactoring is not worth it, makes the code harder to read or the performance implication are too negative. This shows that data clumps are a code smell where there is a wide disagreement on when refactoring is warranted and when not. because all tested projects have a multitude of different data clumps that have various sizes, affected files or included variables, it is very difficult to select a suitable data clump. Traditional metrics like the size or the occurrence seem to have only a partial influence on the acceptance of data clump refactoring. Instead, the relationship of the variables is more often a better indicator. Here, the model can be a great help as it does not necessarily rely on these common metrics. However, as it can bee seen, the choice by the model is often not enough. Instead, the inherent knowledge of the contributors of a project is a major factor in  deciding whether refactoring is warranted. Only with information about the scope, goals, and issues of a project, it is possible to determine where refactoring is more likely to be warranted. Even with the help of \\ac{LLM}, external contributions can hardly mitigate this lack of knowledge which explains many pull request rejections. \n\nAnother highlighted issue is the legal implication of using \\acp{LLM} which is a significant concern for open source developers. Because large language models derive their knowledge from a multitude of sources, cannot give these sources adequately and are relatively new, the developers are too cautious to integrate refactorings by \\acp{LLM}.\"Even without knowing that a traditional approach was used for the actual refactoring, merely mentioning the use of LLMs led to rejections.  This indicates that the legal implications of these models are fundamental and need to be resolved before they can be accepted. \n\n\n\n\\newcommand{\\dolphinscheduler}{\\textit{DolphinScheduler}}\n\\newcommand{\\rocketmq}{\\textit{RocketMQ}}\n\\newcommand{\\argouml}{\\textit{ArgoUML}}\n\n\\dolphinscheduler\n\\"
    },
    {
      "score": 2359,
      "sentence": "Threats to validity}\n\nThe survey evaluation has some threats to its validity that should not be disregarded.\n\nTo begin with, the selection of GitHub project is not completely unbiased. For instance, currently trending GitHub projects were chosen although it is not certain why they were popular. Therefore, the maintainers of the project might not be fully prepared to deal with associated surge in pull requests associated with trending projects. This can increase the chance that a pull request (even though reasonable and well-meant) is summarily rejected. \\cite{10.1145/3366423.3380272}\n\nAdditionally, only projects that do build flawlessly were considered. Usually, this should be case for every project, but the reality differs. For instance, the operating system, the installed libraries, the Java version, and other components can have a significant impact on whether all unit tests complete without errors and the project builds.  While every effort was made to give each project a chance to compile, at some point, time constraints prevented endless consideration of a project. Hence, if even after testing on multiple system, reading the associated documentation, and testing multiple Java version, no fully functional build was achieved, the project was disregarded.\n\nIt is also important to consider the manual correction process, as it could introduce mistakes or bugs due to human error rather than errors from the \\ac{LLM}. Feedback regarding these errors must be filtered out as they are not relevant. \n\n\nAdditionally, the decision to accept or reject a pull request can depend on many factors. For instance, the current mood of participating developers can influence the rejection rate \\cite{detecting_emotional}.\n\nFor larger project, bureaucracy was also a factor that prevented the creation of pull requests. For instance, some projects require that pull request must always relate to an existing issue so that the general idea can be discussed without providing source code. For usual contributors, this might be beneficial as they do not need to spend time on writing code that is rejected in the end because the developers do not like the general approach. For the purpose of this evaluation, the actual source generated is essential, so creating an issue first would be possible,  but more burdensome  and would not add much value. Nevertheless, the sole existence of such rules was not a criterion to filter out a project. \n\n\\subsub"
    },
    {
      "score": 2162,
      "sentence": "DataClumpDoctor}\n\nThe main tool to detect  data clumps is the \\textit{DataClumpDoctor}. As this program is written using the NodeJS platform it can be directly integrated into the main program blurring distinction between handler and service.\n\nThe \\textit{DataClumpDoctor} performs both the \\ac{AST} generation step and the data clump detection step although these two steps are internally separated.\n\nIf a \\ac{AST} context is already present, it is re-used. Otherwise, the \\ac{AST} context is generated. Here, the file filtering inclusion and exclusion rules must be applied as only the generated \\ac{AST} files form the foundation for the \\textit{DataClumpDoctor} to detect data clumps. Files  not represented by an \\ac{AST} file are ignored. Since the detector internally uses \\textit{PMD}, inclusion and exclusion rules can be set by creating a \\enquote{custom-java-ruleset.xml} file. This file is located inside the Java-Archive file used by the detector and must be updated whenever \\ac{AST} files are generated to apply the current filtering rules. \n\nAfter that, the \\textit{DataClumpDoctor} loads the \\ac{AST} files and detects the data clumps by comparing the identifiers and types of variables that might constitute a data clump, thereby requiring strict equality. These detected data clumps are saved in a separate file and can be re-used in the pipeline. \n\nOne issue with the \\textit{DataClumpDoctor} is its handling of large projects. While a concrete \\ac{LOC} number cannot be given, at some points analyzing large projects becomes infeasible leading to crashes or prolonged running times. This can be explained by considering that  the number of detected data clumps grows quadratically with the actual number of data clump holders. Due to the structure of the reporting format as discussed in appendix \\ref{app:data_clump_format}, there is a significant amount of repetition. For sufficiently large projects, this can be a problem it the data clump information needs to be serialized as NodeJS is not optimized for these tasks. While changing the format is one solution to this problem, it can be argued that using previous file filters is more suitable as the data clump detection results from large projects are difficult to evaluate. \n\n\\sub"
    },
    {
      "score": 1948,
      "sentence": "Handling invalid \\ac{JSON} objects}\n\nWhen using the \\ac{JSON} output format, it is essential to deal with edge cases where the output is not valid \\ac{JSON}. This problem can happen in two scenarios.\n\nFirstly, not all \\acp{LLM} support \\ac{JSON}-only modes. This means that that the content of the model cannot be forced to contain only \\ac{JSON} but might contain additional text that cannot be parsed. While the model used in this thesis supports this output mode, it is useful to keep these edge cases in mind. This edge case can be circumvented by finding the first opening curly brace \\enquote{\\{} in the response and the last closing curly brace \\enquote{\\}} because the string between should be valid \\ac{JSON}. As a result, any text before or after the \\ac{JSON} is ignored and cut off. \n\nSecondly, one problem for \\acp{LLM} is that they generate token by token and therefore cannot pre-plan their output. Usually, this does not cause problems. \n\nHowever, as outlined in section \\ref{sec:llm_challenges}, the maximum output size of a model is restricted so that a model cannot generate large output at once. The model can be prompted to continue the output, but this requires additional prompts. Additionally, these interim prompts must be combined into one so that they can parsed.\n\nIf the output format is \\ac{JSON}, handling interruptions can become problematic because \\ac{JSON} syntax requires a precise structure. For example, if the closing curly braces are missing, standard \\ac{JSON} parsers will struggle to process the data and will typically generate an error. This strict syntactic requirement means that even minor deviations can disrupt \\ac{JSON} data handling.\n\nThe easiest solution to this problem is to simply ignore the output as the response was invalid. \nHowever,  one can still try to parse the output as the incomplete \\ac{JSON} might still contain relevant data (e.~g. refactoring instructions). While the refactoring is probably incomplete, it might be a good start and possible errors can be fixed in the validation step.\n\n\n\\sub"
    }
  ]
}